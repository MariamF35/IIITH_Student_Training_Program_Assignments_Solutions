Solutions:


1. Why is Linear Algebra important in machine learning?
   It provides the foundation for handling data as vectors and matrices, enabling operations like transformations, optimization, and training of models such as neural networks.

2. What is a Matrix Transpose and what does it do?
   It flips a matrix over. The rows become columns and columns become rows.

3. What is the Dot Product used for?
   It measures similarity between vectors and is used in projections, gradient calculations, and model weight updates.

4. When are two matrices compatible for multiplication?
   When the number of columns in the first matrix equals the number of rows in the second.

5. Which is more efficient — `np.linalg.multi_dot()` or chaining `.dot()` — and why?
   `multi_dot()` is faster because it automatically chooses the most efficient multiplication order to minimize computations.

6. What is the difference between inner and outer product?

   * Inner product: returns a scalar or matrix showing similarity.
   * Outer product: returns a full matrix from all pairwise multiplications of elements.

7. What does the determinant of a matrix tell you?
   It indicates if the matrix is invertible and represents scaling or volume change under transformation.

8. Why do we need the inverse of a matrix?
   It’s used to solve linear systems and undo transformations.mber.

9. What are Eigenvalues and Eigenvectors used for?
   They describe directions (vectors) and magnitudes (values) that remain unchanged under a transformation — used in PCA, stability analysis, and dimensionality reduction.

10. What is SVD and how can it be applied to image compression?
    Singular Value Decomposition breaks a matrix into three parts (U, S, Vᵀ).
    By keeping only top *k* singular values, we approximate the image using fewer components, reducing storage while preserving quality.


Programs: 

1. Matrix Transpose Program
   # Using numpy array
   A = np.array([[1,2,3],[4,5,6]])
   A_T = A.T
  np.linalg.matrix_power(A, -1)
   ```

9. Eigenvalues and Eigenvectors

   ```python
   vals, vecs = np.linalg.eig(A)
   ```

10. Trace and Norm

    ```python
    np.trace(A)
    np.linalg.norm(A, 'fro')
    ```

11. Solve Linear System

    ```python
    x = np.linalg.solve(A, b)
    ```

12. SVD Image Compression

    * Load grayscale image.
    * Perform `U, S, Vt = np.linalg.svd(img)`.
    * Reconstruct with top *k* components.
    * Compare quality for `k=50`, `k=20`.

13. Error Handling

    * Try multiplying incompatible matrices → observe error.
    * Fix by transposing one.
